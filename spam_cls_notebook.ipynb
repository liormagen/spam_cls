{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from nltk import SnowballStemmer, re, downloader\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "current_path = os.getcwd()\n",
    "data_path = os.path.join(current_path, 'data', 'interviewClassificationTask.csv')\n",
    "train = pd.read_csv(data_path, encoding='ISO-8859-1')\n",
    "\n",
    "fields = ['v1', 'v2_concat']\n",
    "# Make sure no cell stays empty\n",
    "x_train_ = train[fields[1]].fillna(\"fillna\").values.tolist() \n",
    "y_train = train[fields[0]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Characterize the data - Useful functions\n",
    "filters = '!\"#%&()*+,-./:;<=>?_@[\\\\]^`{|}~\\t\\n0123456789'\n",
    "\n",
    "\n",
    "def clean_sw():\n",
    "    try:\n",
    "        sw = stopwords.words('english')\n",
    "    except LookupError:\n",
    "        downloader.download('stopwords')\n",
    "        sw = stopwords.words('english')\n",
    "    return set([english_stemmer(w) for w in sw])\n",
    "\n",
    "\n",
    "def english_stemmer(word):\n",
    "    stemmed_word = SnowballStemmer('english').stem(word)\n",
    "    return stemmed_word\n",
    "\n",
    "\n",
    "def strip_url(text, return_count=False):\n",
    "    urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text)\n",
    "    if return_count:\n",
    "        return len(urls)\n",
    "    for url in urls:\n",
    "        text = text.replace(url, '_URL_')\n",
    "    text = text.replace('https:', '')\n",
    "    return text\n",
    "\n",
    "\n",
    "def convert_emphesize(text, return_count=False):\n",
    "    emphs = re.findall(r'\\b[A-Z]{2,}\\b', text)\n",
    "    emphs = set(emphs)\n",
    "    if return_count:\n",
    "        return len(emphs)\n",
    "    for emph_ in emphs:\n",
    "        text = re.sub(r'\\b' + emph_ + r'\\b', emph_ + ' emphh', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def trivial_tokenizer(text):\n",
    "    return text\n",
    "\n",
    "\n",
    "def is_long_number(text, threshold=1, flag_res=False):\n",
    "    numbers_lens = re.findall('\\\\d+', text)\n",
    "    if numbers_lens and len(max(numbers_lens, key=len)) >= threshold:\n",
    "        if flag_res:\n",
    "            return len(max(numbers_lens, key=len))\n",
    "        return text + ' _longnumber_'\n",
    "    if flag_res:\n",
    "        return 0\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Characterize the data\n",
    "\n",
    "positive_class = 'not_spam'\n",
    "negative_class = 'spam'\n",
    "class_convert = {1: [negative_class], 0: [positive_class]}\n",
    "\n",
    "spam_number_counter = []\n",
    "non_spam_number_counter = []\n",
    "\n",
    "spam_emph, non_spam_emph = [], []\n",
    "spam_urls, non_spam_urls = [], []\n",
    "\n",
    "non_spam_docs_count = 0\n",
    "spam_docs_count = 0\n",
    "\n",
    "spam_money_count = 0\n",
    "non_spam_money_count = 0\n",
    "\n",
    "for x_, y_ in zip(x_train_, y_train):\n",
    "    emph_count = convert_emphesize(x_, return_count=True)\n",
    "    if emph_count:\n",
    "        ratio_ = emph_count\n",
    "        if (y_ == [0]).all():\n",
    "            non_spam_emph.append(ratio_)\n",
    "        else:\n",
    "            spam_emph.append(ratio_)\n",
    "\n",
    "    if (y_ == [0]).all():\n",
    "        if '£' in x_:\n",
    "            non_spam_money_count += 1\n",
    "        non_spam_docs_count += 1\n",
    "        non_spam_number_counter.append(is_long_number(x_, flag_res=True))\n",
    "        non_spam_urls.append(strip_url(x_, return_count=True))\n",
    "    else:\n",
    "        if '£' in x_:\n",
    "            spam_money_count += 1\n",
    "        spam_docs_count += 1\n",
    "        spam_number_counter.append(is_long_number(x_, flag_res=True))\n",
    "        spam_urls.append(strip_url(x_, return_count=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is there a link between long numbers in text and spam messages?\n",
    "print('Number of occurrences of long (more than 4 following digits) numbers in non-spam sentences: %s/%s'\n",
    "      % (sum(i > 4 for i in non_spam_number_counter), non_spam_docs_count))\n",
    "print('Number of occurrences of long (more than 4 following digits) numbers in spam sentences: %s/%s\\n' %\n",
    "      (sum(i > 4 for i in spam_number_counter), spam_docs_count))\n",
    "\n",
    "print('Number of occurrences of long (more than 3 following digits) numbers in non-spam sentences: %s/%s'\n",
    "      % (sum(i > 3 for i in non_spam_number_counter), non_spam_docs_count))\n",
    "print('Number of occurrences of long (more than 3 following digits) numbers in spam sentences: %s/%s\\n' %\n",
    "      (sum(i > 3 for i in spam_number_counter), spam_docs_count))\n",
    "\n",
    "print('Number of occurrences of long (more than 2 following digits) numbers in non-spam sentences: %s/%s'\n",
    "      % (sum(i > 2 for i in non_spam_number_counter), non_spam_docs_count))\n",
    "print('Number of occurrences of long (more than 2 following digits) numbers in spam sentences: %s/%s\\n' %\n",
    "      (sum(i > 2 for i in spam_number_counter), spam_docs_count))\n",
    "\n",
    "print('Based on the fact that in most of the spam messages there are long numbers (we''l stick to emphasizing number '\n",
    "      'that are longer than 4 digits) we will add a special terms that will contain this information as part of the '\n",
    "      'text')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is there a link between Pound sign and spam messages?\n",
    "print('Money sign (Pound) occurred in %s/%s spam docs' % (spam_money_count, spam_docs_count))\n",
    "print('Money sign (Pound) occurred in %s/%s non-spam docs' % (non_spam_money_count, non_spam_docs_count))\n",
    "\n",
    "print('Based on the fact that in most spam messages we see a Pound sign, we''ll keep this special '\n",
    "      'sign for better classification results\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is there a link between URLs and spam messages?\n",
    "print('\\nURLs exists in %s/%s of the spam docs' % (sum(i > 0 for i in spam_urls), spam_docs_count))\n",
    "print('URLs exists in %s/%s of the non-spam docs' % (sum(i > 0 for i in non_spam_urls), non_spam_docs_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is there a link between emphasized words and spam messages?\n",
    "print('Trying to prove that spam messages contains more emphasized words than non-spam messages')\n",
    "fig1 = plt.figure(1)\n",
    "plt.hist(spam_emph)\n",
    "plt.title('Spam emphasized words histogram - avg=%.3f, std=%.3f' % (statistics.mean(spam_emph),\n",
    "                                                                    statistics.stdev(spam_emph)))\n",
    "plt.xlabel('Ratio')\n",
    "plt.ylabel('Count')\n",
    "fig1.show()\n",
    "\n",
    "fig2 = plt.figure(2)\n",
    "plt.title('Not-spam emphasized words histogram - avg=%.3f, std=%.3f' % (statistics.mean(non_spam_emph),\n",
    "                                                                        statistics.stdev(non_spam_emph)))\n",
    "plt.hist(non_spam_emph)\n",
    "plt.xlabel('Ratio')\n",
    "plt.ylabel('Count')\n",
    "fig2.show()\n",
    "\n",
    "fig3 = plt.figure(3)\n",
    "plt.hist(spam_emph, label='Spam')\n",
    "plt.hist(non_spam_emph, label='Not spam')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Emphasized words count - Spam VS not-spam')\n",
    "fig3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PreProcessing the data\n",
    "# Here we'll clean the data, tokenize it, stem it, emphasize certain entities (based on the previous section)\n",
    "stem_it = True\n",
    "sw = clean_sw()\n",
    "lengths = 0\n",
    "max_features = 800\n",
    "\n",
    "pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL)\n",
    "\n",
    "for idx, doc in enumerate(x_train_):\n",
    "    doc = strip_url(doc)  # Replace URLs with a special term (reducing the amount of features while still \n",
    "    # not loosing the fact that there's a URL in the text\n",
    "    doc = is_long_number(doc)  # The tokenizer filters numbers so we'll use this function to keep the knowledge that\n",
    "    # a long number exists in the text\n",
    "    doc = pattern.sub(r\"\\1\", doc)  # Replacing repeating chars with the original version (loooooove -> love)\n",
    "    doc = convert_emphesize(doc)  # We'll add a constant term next to emphasized words ('LOVE' -> 'love emphh'). \n",
    "    # In the pre processing phase we're converting everything to lower case and we'll loose this important knowledge\n",
    "    # , keeping this knowledge may improve the classifier results. Emotional people tend to use emphasized words. \n",
    "    tokens = [english_stemmer(w) for w in text_to_word_sequence(doc, filters=filters, lower=True)]  # Cleaning some\n",
    "    # chars, tokenizing and stemming the text (less features: 'love', 'loving', 'loves' will all become 'love').\n",
    "    x_train_[idx] = [w for w in tokens if w not in sw]  # Cleaning stop words (in most cases stop words are totally\n",
    "    # redundant).\n",
    "    lengths += len(x_train_[idx])\n",
    "\n",
    "max_len = round(lengths / idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly split data to train and test. It is possible and common to repeat this split a few times, training a few\n",
    "# models and process with the best one. In this case I chose to do it once, the results are good enough.\n",
    "test_size = .2\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train_, y_train, test_size=test_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "# We're dealing with a small quantity of data so I decided to use a linear SVM (binary classifier). Linear SVM \n",
    "# deals well with text. \n",
    "# We'll use a pipeline that our data will go through - a vectorizer, TFIDF and than the selected classifier\n",
    "input_vectorizer = CountVectorizer(tokenizer=trivial_tokenizer, max_features=max_features, lowercase=False)\n",
    "tfidf = TfidfTransformer()\n",
    "linSVC = LinearSVC()\n",
    "pipeline = [('Vectorizer', input_vectorizer), ('TFIDF', tfidf), ('LinSVC', linSVC)]\n",
    "model = Pipeline(pipeline)\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the test set and comparing predictions and actual values\n",
    "y_predicted = model.predict(x_test)\n",
    "print(classification_report(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary \n",
    "# The amount of data in this task is really small if it was a case of dealing massive amounts of data I would\n",
    "# have done most of the part (pre processing/training/predicting) in parallel (each one of them of course)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
